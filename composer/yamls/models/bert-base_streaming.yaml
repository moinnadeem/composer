train_dataset:
  c4:
    split: train
    num_samples: 275184000 
    tokenizer_name: bert-base-uncased
    max_seq_len: 128 
    group_method: truncate # TODO: check that the code for this one is right
    mlm: true # TODO: check that the code for this one is right 
    mlm_probability: 0.15
    shuffle: true
    seed: 17
    drop_last: true
evaluators:
  evaluator:
    label: bert_pre_training
    eval_dataset:
      c4:
        split: validation
        num_samples: 32000 # there's a bug, this is currently per-device 
        tokenizer_name: bert-base-uncased 
        max_seq_len: 128 
        group_method: truncate # TODO: check that the code for this one is right
        mlm: true # TODO: check that the code for this one is right
        mlm_probability: 0.15
        shuffle: false
        seed: 17
        drop_last: true
    metric_names:
      - LanguageCrossEntropy
      - MaskedAccuracy
model:
  bert:
    use_pretrained: false
    tokenizer_name: bert-base-uncased
    pretrained_model_name: bert-base-uncased
optimizer:
  decoupled_adamw:
    lr: 5.0e-4
    betas:
      - 0.9
      - 0.98
    eps: 1.0e-06
    weight_decay: 1.0e-5
schedulers:
  - linear_decay_with_warmup:
      t_warmup: 0.06dur
loggers:
  - progress_bar: {}
max_duration: 1ep  # Baseline is 256M samples, 7 epochs is ~280M samples
log_level: INFO 
train_batch_size: 4000
eval_batch_size: 2000
seed: 19
device:
  gpu: {}
dataloader:
  pin_memory: true
  persistent_workers: true
  num_workers: 1
  timeout: 0
  prefetch_factor: 2
grad_accum: 1
precision: amp
grad_clip_norm: None
validate_every_n_batches: 1000
validate_every_n_epochs: 1
save_folder: bert_checkpoints
save_interval: 3500ba 
