---
run_name: bert-baseline
gpu_type: a100_80gb
gpu_num: 8
platform: r1z1
image: mosaicml/pytorch_internal:latest
integrations:
  - integration_type: "wandb"
    project: bert-data-collection
  - integration_type: "git_repo"
    git_repo: mosaicml/composer
    git_branch: moin/orig_streaming_dataset
    pip_install: .[all]
command: >-
  set -e -x

  cd composer

  composer examples/run_composer_trainer.py -f /mnt/config/parameters.yaml
parameters:
  train_dataset:
    streaming_lm:
      dataset_name: c4
      dataset_config_name: en
      split: train
      max_samples: 275184000
      max_seq_len: 128
      group_method: truncate  # TODO: check that the code for this one is right
      tokenizer_name: bert-base-uncased
      use_masked_lm: true  # TODO: check that the code for this one is right
      mlm_probability: 0.15
      seed: 17
      shuffle: true
      drop_last: true
  evaluators:
    evaluator:
      label: bert_pre_training
      eval_dataset:
        streaming_lm:
          dataset_name: c4
          dataset_config_name: en
          split: validation
          max_samples: 32000
          max_seq_len: 128
          group_method: truncate
          tokenizer_name: bert-base-uncased
          use_masked_lm: true
          mlm_probability: 0.15
          seed: 17
          shuffle: false
          drop_last: true
      metric_names:
        - LanguageCrossEntropy
        - MaskedAccuracy
  model:
    bert:
      use_pretrained: false
      tokenizer_name: bert-base-uncased
      pretrained_model_name: bert-base-uncased
  optimizer:
    decoupled_adamw:
      lr: 5.0e-4
      betas:
        - 0.9
        - 0.98
      eps: 1.0e-06
      weight_decay: 1.0e-5
  schedulers:
    - linear_decay_with_warmup:
        t_warmup: 0.06dur
        alpha_f: 0.02
  loggers:
    - progress_bar: {}
    - wandb: {}
    - object_store:
        object_store_hparams:
          provider: GOOGLE_STORAGE
          container:   # TODO: the user should fill this in
          key_environ: GCS_KEY
          secret_environ: GCS_SECRET
  callbacks:
    - speed_monitor:
        window_size: 500
    - lr_monitor: {}
  max_duration: 1ep
  train_batch_size: 4000
  eval_batch_size: 2000
  seed: 3407
  device:
    gpu: {}
  dataloader:
    pin_memory: true
    persistent_workers: true
    num_workers: 1
    timeout: 0
    prefetch_factor: 2
  precision: amp
  grad_clip_norm: -1.0
  eval_interval: 3500ba
  save_folder: bert_checkpoints
  save_interval: 3500ba
  save_artifact_name: "{run_name}/checkpoints/ep{epoch}-ba{batch}-rank{rank}-{total_wct}wct"
  grad_accum: 1
