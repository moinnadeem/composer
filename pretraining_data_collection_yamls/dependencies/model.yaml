parameters:
  train_dataset:
    streaming_lm:
      dataset_name: c4
      dataset_config_name: en
      split: train
      max_samples: 275184000
      max_seq_len: 128
      group_method: truncate  # TODO: check that the code for this one is right
      tokenizer_name: bert-base-uncased
      use_masked_lm: true  # TODO: check that the code for this one is right
      mlm_probability: 0.15
      seed: 17
      shuffle: true
      drop_last: true
  evaluators:
    evaluator:
      label: bert_pre_training
      eval_dataset:
        streaming_lm:
          dataset_name: c4
          dataset_config_name: en
          split: validation
          max_samples: 32000
          max_seq_len: 128
          group_method: truncate
          tokenizer_name: bert-base-uncased
          use_masked_lm: true
          mlm_probability: 0.15
          seed: 17
          shuffle: false
          drop_last: true
      metric_names:
        - LanguageCrossEntropy
        - MaskedAccuracy
  model:
    bert:
      use_pretrained: false
      tokenizer_name: bert-base-uncased
      pretrained_model_name: bert-base-uncased
  optimizer:
    decoupled_adamw:
      lr: 5.0e-4
      betas:
        - 0.9
        - 0.98
      eps: 1.0e-06
      weight_decay: 1.0e-5
  schedulers:
    - linear_decay_with_warmup:
        t_warmup: 0.06dur
        alpha_f: 0.02
  loggers:
    - progress_bar: {}
    - wandb: {}
    - object_store:
        object_store_hparams:
          provider: GOOGLE_STORAGE
          container: bert_gold_checkpoints
          key_environ: GCS_KEY
          secret_environ: GCS_SECRET
  callbacks:
    - speed_monitor:
        window_size: 500
    - lr_monitor: {}
  max_duration: 1ep  # Baseline is 256M samples, 7 epochs is ~280M samples
  train_batch_size: 4000
  eval_batch_size: 2000
  seed: 3407
  device:
    gpu: {}
  dataloader:
    pin_memory: true
    persistent_workers: true
    num_workers: 1
    timeout: 0
    prefetch_factor: 2
  grad_accum: 1
  precision: amp
  grad_clip_norm: -1.0
  eval_interval: 3500ba
  save_folder: bert_checkpoints
  save_interval: 3500ba
  run_name: "{job[run_name]}"
