command: 'set -e -x

  git clone http://github.com/moinnadeem/composer /root/composer

  cd /root/composer

  echo ''Checking out composer branch {{ git_branch }}''

  git checkout moin/cleaned_gated

  pip install --user -e ".[all]"

  composer examples/run_composer_trainer.py -f /mnt/config/parameters.yaml'
gpu_num: 8
gpu_type: a100_80gb
image: mosaicml/moin
integrations:
- integration_type: wandb
  project: bert-pareto-curves
parameters:
  algorithms:
    act_fn_search:
      act_fn_name: gelu
      use_fln: false
      use_gated: true
      use_rmsnorm: false
      use_triton: false
      w0_bias: false
      w1_bias: false
  callbacks:
  - speed_monitor:
      window_size: 500
  - lr_monitor: {}
  dataloader:
    num_workers: 1
    persistent_workers: true
    pin_memory: true
    prefetch_factor: 2
    timeout: 0
  device:
    gpu: {}
  eval_batch_size: 2000
  eval_interval: 3500ba
  evaluators:
    evaluator:
      eval_dataset:
        streaming_lm:
          dataset_config_name: en
          dataset_name: c4
          drop_last: true
          group_method: truncate
          max_samples: 32000
          max_seq_len: 128
          mlm_probability: 0.15
          seed: 17
          shuffle: false
          split: validation
          tokenizer_name: bert-base-uncased
          use_masked_lm: true
      label: bert_pre_training
      metric_names:
      - LanguageCrossEntropy
      - MaskedAccuracy
  grad_accum: 1
  grad_clip_norm: -1.0
  loggers:
  - progress_bar: {}
  - wandb: {}
  - object_store:
      object_store_hparams:
        container: bert_gold_checkpoints
        key_environ: GCS_KEY
        provider: GOOGLE_STORAGE
        secret_environ: GCS_SECRET
  max_duration: 1ep
  model:
    bert:
      pretrained_model_name: bert-base-uncased
      tokenizer_name: bert-base-uncased
      use_pretrained: false
  optimizer:
    decoupled_adamw:
      betas:
      - 0.9
      - 0.98
      eps: 1.0e-06
      lr: 0.0005
      weight_decay: 1.0e-05
  precision: amp
  run_name: moin-bert-gelu-gated
  save_artifact_name: '{run_name}/checkpoints/ep{epoch}-ba{batch}-rank{rank}-{total_wct}wct'
  save_folder: bert_checkpoints
  save_interval: 3500ba
  schedulers:
  - linear_decay_with_warmup:
      alpha_f: 0.02
      t_warmup: 0.06dur
  seed: 3407
  train_batch_size: 4000
  train_dataset:
    streaming_lm:
      dataset_config_name: en
      dataset_name: c4
      drop_last: true
      group_method: truncate
      max_samples: 275184000
      max_seq_len: 128
      mlm_probability: 0.15
      seed: 17
      shuffle: true
      split: train
      tokenizer_name: bert-base-uncased
      use_masked_lm: true
platform: r1z1
run_name: moin-bert-gelu-gated
